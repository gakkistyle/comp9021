{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does not need to be executed if\n",
    "# ~/.ipython/profile_default/ipython_config.py\n",
    "# exists and contains:\n",
    "# get_config().InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict, deque\n",
    "from copy import copy, deepcopy\n",
    "from itertools import islice\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A _definite logic program_ consists of _definite clauses_, that is, _universal closures_ of implications whose right hand sides are _atomic formulas_ and whose left hand sides are conjunctions of atomic formulas. Because a conjunction over an empty set of formulas is logically true, and an implication whose left hand side is a tautology is logically equivalent to its right hand side, atomic formulas, also known as _atoms_ or _facts_, are particular cases of definite clauses. Here is a definite logic program consisting of 13 definite clauses, the first 8 of which are facts:  \n",
    "\n",
    "$\\mathrm{father(bob,jack)}$  \n",
    "$\\mathrm{father(bob,sandra)}$  \n",
    "$\\mathrm{father(john,bob)}$  \n",
    "$\\mathrm{father(john,mary)}$  \n",
    "$\\mathrm{mother(jane,jack)}$  \n",
    "$\\mathrm{mother(jane,sandra)}$  \n",
    "$\\mathrm{mother(emily,bob)}$  \n",
    "$\\mathrm{mother(emily,mary)}$  \n",
    "$\\mathrm{\\forall X\\forall Y\\bigl(father(X,Y)\\rightarrow parent(X,Y)\\bigr)}$  \n",
    "$\\mathrm{\\forall X\\forall Y\\bigl(mother(X,Y)\\rightarrow parent(X,Y)\\bigr)}$  \n",
    "$\\mathrm{\\forall X\\forall Y\\bigl(parent(Y,X)\\wedge male(X)\\rightarrow son(X,Y)\\bigr)}$  \n",
    "$\\mathrm{\\forall X\\forall Y\\bigl(parent(Y,X)\\wedge female(X)\\rightarrow daughter(X,Y)\\bigr)}$  \n",
    "$\\mathrm{\\forall X\\forall Y\\forall Z\\bigl(male(X)\\wedge parent(Z,X)\\wedge parent(Z,Y)\\rightarrow brother(X,Y)\\bigr)}$  \n",
    "$\\mathrm{\\forall X\\forall Y\\forall Z\\bigl(parent(X,Z)\\wedge parent(Z,Y)\\rightarrow grandparent(X,Y)\\bigr)}$\n",
    "\n",
    "In English, this reads as:\n",
    "\n",
    "* Bob is a father of Jack and Sandra, John is a father of Bob and Mary, Jane is a mother of Jack and Sandra, Emily is a mother of Bob and Mary.\n",
    "* For all X, for all Y, if X is a father of Y then X is a parent of Y.\n",
    "* For all X, for all Y, if X is a mother of Y then X is a parent of Y.\n",
    "* For all X, for all Y, if Y is a parent of X and X is male then X is a son of Y.\n",
    "* For all X, for all Y, if Y is a parent of X and X is female then X is a daughter of Y.\n",
    "* For all X, for all Y, for all Z, if X is male, Z is a parent of X and Z is a parent of Y then X is is a brother of Y.\n",
    "* For all X, for all Y, for all Z, if X is a parent of Z and Z is a parent of Y, then X is a grandparent of Y.\n",
    "\n",
    "Note that the last two definite clauses are logically equivalent to:\n",
    "\n",
    "$\\mathrm{\\forall X\\forall Y\\Bigl(male(X)\\wedge\\exists Z\\bigl(parent(Z,X)\\wedge parent(Z,Y)\\bigr)\\rightarrow brother(X,Y)\\Bigr)}$  \n",
    "$\\mathrm{\\forall X\\forall Y\\Bigl(\\exists Z\\bigl(parent(X,Z)\\wedge parent(Z,Y)\\bigr)\\rightarrow grandparent(X,Y)\\Bigr)}$\n",
    "\n",
    "which reads as:\n",
    "\n",
    "* For all X, for all Y, if X is male and there exists Z such that Z is a parent of X and Z is a parent of Y, then X is is a brother of Y.\n",
    "* For all X, for all Y, if there exists Z such that X is a parent of Z and Z is a parent of Y, then X is a grandparent of Y.\n",
    "\n",
    "In Prolog, this definite logic program takes the form:\n",
    "\n",
    "father(bob, jack).  \n",
    "father(bob, sandra).  \n",
    "father(john, bob).  \n",
    "father(john, mary).  \n",
    "mother(jane, jack).  \n",
    "mother(jane, sandra).  \n",
    "mother(emily, bob).  \n",
    "mother(emily, mary).  \n",
    "parent(X, Y) :- father(X, Y).  \n",
    "parent(X, Y) :- mother(X, Y).  \n",
    "son(X, Y) :- parent(Y, X), male(X).   \n",
    "daughter(X, Y) :- parent(Y, X), female(X).  \n",
    "brother(X, Y) :- male(X), parent(Z, X), parent(Z, Y).  \n",
    "grandparent(X, Y) :- parent(X, Z), parent(Z, Y).\n",
    "\n",
    "An English reading of the definite clauses that are not facts that more closely follows this alternative syntax is:\n",
    "\n",
    "* For all X, for all Y, for X to be a parent of Y, it suffices that X be a father of Y.\n",
    "* For all X, for all Y, for X to be a parent of Y, it suffices that X be a mother of Y.\n",
    "* For all X, for all Y, for X to be a son of Y, it suffices that Y be a parent of X and that X be male.\n",
    "* For all X, for all Y, for X to be a daughter of Y, it suffices that Y be a parent of X and that X be female.\n",
    "* For all X, for all Y, for X to be a brother of Y, it suffices that X be male and that some Z exists that is a parent of both X and Y.\n",
    "* For all X, for all Y, for X to be a grandparent of Y, it suffices that some Z exists such that X is a parent of Z and Z is a parent of Y.\n",
    "\n",
    "The logical syntax suggests successive applications of _instantiation_ and _modus ponens_ to derive some atoms such as $\\mathrm{grandparent(john,jack)}$ from the given facts, in a _bottom up_ manner:\n",
    "\n",
    "* It is known that $\\mathrm{father(bob,jack)}$. Together with $\\mathrm{\\forall X\\forall Y\\bigl(father(X,Y)\\rightarrow parent(X,Y)\\bigr)}$, this implies that $\\mathrm{parent(bob,jack)}$.\n",
    "* It is known that $\\mathrm{father(john,bob)}$. Together with $\\mathrm{\\forall X\\forall Y\\bigl(father(X,Y)\\rightarrow parent(X,Y)\\bigr)}$, this implies that $\\mathrm{parent(john,bob)}$.\n",
    "* From $\\mathrm{\\forall X\\forall Y\\forall Z\\bigl(parent(X,Z)\\wedge parent(Z,Y)\\rightarrow grandparent(X,Y)\\bigr)}$ together with $\\mathrm{parent(john,bob)}$ and $\\mathrm{parent(bob,jack)}$, we infer that $\\mathrm{grandparent(john,jack)}$, ending the proof.\n",
    "\n",
    "The Prolog syntax suggests proving instances of some atomic formulas such as grandparent(john, jack) by proving instances of other atomic formulas that directly imply the latter, until nothing but given facts are eventually reached, in a _top down_ manner:\n",
    "\n",
    "* To prove grandparent(john, jack), it suffices to find a value for Z and prove both parent(john, Z) and parent(Z, jack).\n",
    "  * To find a value for Z and prove both parent(john, Z) and parent(Z, jack), it suffices to find a value for Z and either prove both father(john, Z) and parent(Z, jack) or prove both mother(john, Z) and parent(Z, jack).\n",
    "    * To find a value for Z and prove both father(john, Z) and parent(Z, jack), one can let Z be either bob or mary as indeed father(john, bob) and father(john, mary) are given, having to then prove either parent(bob, jack) or parent(mary, jack).\n",
    "      * To prove parent(bob, jack), it suffices to prove either father(bob, jack) or mother(bob, jack).\n",
    "        * Indeed, father(bob, jack) is given, ending the proof.\n",
    "         \n",
    "Note that the definite clauses that could be used were selected in the order in which they appeared. Suppose for instance that the clauses of the definite logic program that are not facts were listed as follows in Prolog form:\n",
    "\n",
    "parent(X, Y) :- mother(X, Y).  \n",
    "parent(X, Y) :- father(X, Y).  \n",
    "daughter(X, Y) :- parent(Y, X), female(X).  \n",
    "son(X, Y) :- parent(Y, X), male(X).   \n",
    "brother(X, Y) :- male(X), parent(Z, X), parent(Z, Y).  \n",
    "grandparent(X, Y) :- parent(X, Z), parent(Z, Y).\n",
    "\n",
    "Then the proof of grandparent(john, jack) would proceed as follows:\n",
    "\n",
    "* To prove grandparent(john, jack), it suffices to find a value for Z and prove both parent(john, Z) and parent(Z, jack).\n",
    "  * To find a value for Z and prove both parent(john, Z) and parent(Z, jack), it suffices to find a value for Z and either prove both mother(john, Z) and parent(Z, jack) or prove both father(john, Z) and parent(Z, jack).\n",
    "    * One cannot find a value for Z and prove mother(john, Z).\n",
    "    * To find a value for Z and prove both father(john, Z) and parent(Z, jack), one can let Z be either bob or mary as indeed father(john, bob) and father(john, mary) are given, having to then prove either parent(bob, jack) or parent(mary, jack).\n",
    "      * To prove parent(bob, jack), it suffices to prove either mother(bob, jack) or father(bob, jack).\n",
    "        * mother(bob, jack) cannot be proved.\n",
    "        * Indeed, father(bob, jack) is given, ending the proof.\n",
    "\n",
    "grandparent(john, jack) is a _closed_ formula: it contains no variable. Working with formulas that do contain variables, the approach that has been described lets us do more than prove: it lets us compute. Rather than talking about proving an atomic formula, one talks about _solving a goal_ or _answering a query_. For instance, solving the goal grandparent(john, X) lets us compute the instantiations of X that make the resulting formula a logical consequence of the logic program in its logical form. With the original ordering of the clauses, this proceeds as follows:\n",
    "\n",
    "* To solve grandparent(john, X), it suffices to find values for Z and Y and solve both parent(john, Z) and parent(Z, Y). One can then give X the value of Y.\n",
    "  * To find values for Z and Y and solve both parent(john, Z) and parent(Z, Y), it suffices to find values for Y and Y_0 and either solve both father(john, Y_0) and parent(Y_0, Y) or solve both mother(john, Y_0) and parent(Y_0, Y). One can then give X the value of Y.\n",
    "    * To find values for Y and Y_0 and solve both father(john, Y_0) and parent(Y_0, Y), one can let Y_0 be either bob or mary as indeed father(john, bob) and father(john, mary) are given, having to then solve either parent(bob, Y) or parent(mary, Y). One can then give X the value of Y.\n",
    "      * To find a value for Y and solve parent(bob, Y), it suffices to find a value for Y_0 and solve either father(bob, Y_0) or mother(bob, Y_0). One can then give X the value of Y_0.\n",
    "        * To find a value for Y_0 and solve father(bob, Y_0), one can let Y_0 be either jack or sandra. This yields two solutions to the original goal: X can take the value jack, or it can take the value sandra.\n",
    "        * One cannot find a value for Y_0 and solve mother(bob, Y_0).\n",
    "      * To find a value for Y and solve parent(mary, Y), it suffices to find a value for Y_0 and solve either father(mary, Y_0) or mother(mary, Y_0). One can then give X the value of Y_0.\n",
    "        * One cannot find a value for Y_0 and solve father(mary, Y_0).\n",
    "        * One cannot find a value for Y_0 and solve mother(mary, Y_0).\n",
    "  * One cannot find a value for Y_0 and solve mother(john, Y_0).\n",
    "\n",
    "Hence there are two and only two solutions to the goal grandparent(john,X): X equal to jack, and X equal to sandra.\n",
    "\n",
    "There is a bit of mystery in the preceding description in the way some variables are changed or new variables are introduced. This has to do with the fact that different occurrences of a given variable can be unrelated, which requires to sometimes rename variables and introduces somehow tricky technicalities:\n",
    "\n",
    "* Observe that the X in the goal grandparent(john, X) is unrelated to the X in the _rule_ (a more usual name for \"definite clause\" with Prolog notation) grandparent(X, Y) :- parent(X, Z), parent(Z, Y). If we want to let the goal interact with the rule, it is safe to rename all occurrences of X in the rule, using a new variable, say X_0: grandparent(X_0, Y) :- parent(X_0, Z), parent(Z, Y). We can then _unify_ the goal grandparent(john, X) and the _head_ of grandparent(X_0, Y) :- parent(X_0, Z), parent(Z, Y), namely, grandparent(X_0, Y), by replacing X_0 in grandparent(X_0, Y) by john and replacing X in grandparent(john, X) by Y (we could as well replace Y in grandparent(X_0, Y) by X, but the code to be developed later opts for the first alternative). Then the same replacements can be performed in the _body_ of grandparent(X_0, Y) :- parent(X_0, Z), parent(Z, Y), namely, parent(X_0, Z), parent(Z, Y), yielding the rule grandparent(john, Y) :- parent(john, Z), parent(Z, Y). This allows one to replace the goal grandparent(john, X) with the goals parent(john, Z) and parent(Z, Y), knowing that the values of Y that yield solutions to those goals are values of X that yield solutions to the original goal.\n",
    "* Observe that the Y in the goal parent(Z, Y), one of the two goals that we now have to solve, is unrelated to the Y in the rule parent(X, Y) :- father(X, Y). Also, the X in parent(X, Y) :- father(X, Y) is unrelated to the X in the original goal (grandparent(john, X)). If we want to let the other goal we now have to solve, parent(john, Z), interact with that rule, and let the X in the original goal alone, it is safe to rename all occurrences of Y in the rule, using a new variable, say Y_0, and rename all occurrences of X in the rule using a new variable, say X_0: parent(X_0, Y_0) :- father(X_0, Y_0). We can then unify the goal parent(john, Z) and the head of parent(X_0, Y_0) :- father(X_0, Y_0), by replacing X_0 in parent(X_0, Y_0) by john and replacing Z in parent(john, Z) by Y_0. Then the same replacements can be performed in the body of parent(X_0, Y_0) :- father(X_0, Y_0), yielding the rule parent(john, Y_0) :- father(john, Y_0). This allows one to replace the goal parent(john, Z) with the goal father(john, Y_0). But the Z in the goals parent(john, Z) and parent(Z, Y) are related, hence having replaced Z by Y_0 in parent(john, Z), we should replace Z by Y_0 in parent(Z, Y). So we goals we now have to solve are father(john, Y_0) and parent(Y_0, Y).\n",
    "\n",
    "The description of the search for solutions to the goal grandparent(john, X) has the structure of a tree. The following graphical representation of the description makes it particularly clear:\n",
    "\n",
    "![](tree_1.pdf)\n",
    "\n",
    "Observe that with the alternative listing considered above of the clauses of the definite logic program that are not facts, the description of the search for solutions to the goal grandparent(john, X) would be captured by the following tree:\n",
    "\n",
    "![](tree_2.pdf)\n",
    "\n",
    "Instead of starting with one goal, one can start with a sequence of (implicitly conjuncted) goals. For instance, if the logic program was extended with the fact female(sandra), then there would be two solutions to the goals grandparent(john, X), daughter(X, Y): X equal to sandra and Y equal to bob, and X equal to sandra and Y equal to jane.\n",
    "\n",
    "The atoms considered up to now are built from _predicate symbols_ (father, mother, parent, grandparent, male, female, son, daughter, brother), all of _arity_ 2, that is, all _binary_ predicate symbols, _constants_ (bob, jack, sandra, john, mary, jane, emily), and variables. Constant and variables are called _terms_. If we introduce _function symbols_ (of strictly positive arity, as constants are nothing but function symbols of arity 0, or _nullary_ function symbols), then we can build more complex terms. For instance, to represent lists of 0's and 1's, we can use:\n",
    "\n",
    "* 3 constants, o for 0, i for 1, and e for the empty list;\n",
    "* a binary function symbol (function symbol of arity 2) l, whose first argument is meant o or i and represent the first element of a nonempty list, and whose second argument is meant to be a term representing the rest of the list.\n",
    "\n",
    "For instance:\n",
    "\n",
    "* the term l(i, e) represents the list [1].\n",
    "* the term l(o, l(i, e)) represents the list [0, 1].\n",
    "* the term l(o, l(o, l(i, e))) represents the list [0, 0, 1].\n",
    "* the term l(i, l(o, l(o, l(i, e)))) represents the list [1, 0, 0, 1].\n",
    "* the term l(i, l(i, l(o, l(o, l(i, e))))) represents the list [1, 1, 0, 0, 1].\n",
    "\n",
    "Terms such as l(e, e), l(i, o), l(l(i, o), e) and l(l(i, e), l(i, e)) are syntactically valid, but are given no interpretation. \n",
    "\n",
    "We can then consider the logic program that defines the join (concatenation) function:\n",
    "\n",
    "join(e, X, X).  \n",
    "join(l(H, T), X, l(H, Y)) :- join(T, X, Y).\n",
    "\n",
    "This reads as:\n",
    "\n",
    "* Joining the empty list and a list X results in X itself.\n",
    "* To join a nonempty list and a list X, it suffices to join T and X, with T the list consisting of all elements of the first list except the first element H, resulting in a list Y, and put H at the front.\n",
    "\n",
    "This allows us to solve a goal such as join(l(i, l(i, l(o, e))), l(o, l(i, e)), X), with as unique solution X equal to l(i, l(i, l(o, l(o, l(i, e))))), reflecting the fact that joining [1, 1, 0] and [0, 1] results in the list [1, 1, 0, 0, 1]. But one can also solve a goal such as join(X, X, Y), so look for all possible ways to join a list with (a copy of) itself. There are infinitely many solutions, for instance:\n",
    "\n",
    "* X = e and Y = e: joining the empty list with itself results in the empty list.\n",
    "* X = l(o, e) and Y = l(o, l(o, e)): joining [0] with itself results in [0, 0].\n",
    "* X = l(i, e) and Y = l(i, l(i, e)): joining [1] with itself results in [1, 1].\n",
    "* X = l(i, l(o, e)) and Y = l(i, l(o, l(i, l(o, e)))): joining [1, 0] with itself results in [1, 0, 1, 0].\n",
    "\n",
    "Though they are correct, these solutions are not the most general ones, with the exception of the first one: all others are instances of more general solutions, that themselves contain variables:\n",
    "\n",
    "* X equal to l(H, e) and Y equal to l(H, l(H, e)): joining a list of the form [H] (where H is equal to 0 or 1) with itself results in the list [H, H].\n",
    "* X equal to l(H, l(H_0, e)) and Y equal to l(H, l(H_0, l(H, l(H_0, e)))): joining a list of the form [H, H_0] (where H and H_0 are independently equal to 0 or 1) with itself results in the list [H, H_0, H, H_0].\n",
    "\n",
    "By computing _most general unifiers_, one guarantees that nothing but most general solutions be generated. To solve the goal join(X, X, Y):\n",
    "\n",
    "* We first consider the fact join(e, X, X), change it to join(e, X_0, X_0), unifies join(X, X, Y) and join(e, X_0, X_0) with the equalities X = e, X_0 = X and Y = X_0, from which we derive X = e, X_0 = e and Y = e, which yields the solution X = e and Y = e.\n",
    "* We then consider the rule join(l(H, T), X, l(H, Y)) :- join(T, X, Y), change it to join(l(H, T), X_0, l(H, Y_0)) :- join(T, X_0, Y_0), unifies join(X, X, Y) and join(l(H, T), X_0, l(H, Y_0)) with the equalities X = l(H, T), X_0 = X and Y = l(H, Y_0), from which we derive X = l(H, T), X_0 = l(H, T) and Y = l(H, Y_0), having to now solve the goal join(T, l(H, T), Y_0).\n",
    "  * We first consider the fact join(e, X, X), change it to join(e, X_0, X_0), unifies join(T, l(H, T), Y_0) and join(e, X_0, X_0) with the equalities T = e, X_0 = l(H, T) and Y_0 = X_0, from which we derive T = e, X_0 = l(H, e) and Y_0 = l(H, e), which together with X = l(H, T) and Y = l(H, Y_0), yields the solution X = l(H, e) and Y = l(H, l(H, e)).\n",
    "  * We then consider the rule join(l(H, T), X, l(H, Y)) :- join(T, X, Y), change it to join(l(H_0, T_0), X_0, l(H_0, Y_1)) :- join(T_0, X_0, Y_1), unifies join(T, l(H, T), Y_0) and join(l(H_0, T_0), X_0, l(H_0, Y_1)) with the equalities T = l(H_0, T_0), X_0 = l(H, T) and Y_0 = l(H_0, Y_1), from which we derive T = l(H_0, T_0), X_0 = l(H, l(H_0, T_0)), Y_0 = l(H_0, Y_1), X = l(H, l(H_0, T_0)) and Y = l(H, l(H_0, Y_1)), having to now solve the goal join(T_0, l(H, l(H_0, T_0)), Y_1).\n",
    "    * We first consider the fact join(e, X, X), change it to join(e, X_0, X_0), unifies join(T_0, l(H, l(H_0, T_0)), Y_1) and join(e, X_0, X_0) with the equalities T_0 = e, X_0 = l(H, l(H_0, T_0)) and Y_1 = X_0, from which we derive T_0 = e, X_0 = l(H, l(H_0, e)) and Y_1 = l(H, l(H_0, e)), which together with X = l(H, l(H_0, T_0)) and Y = l(H, l(H_0, Y_1)), yields the solution X = l(H, l(H_0, e)) and Y = l(H, l(H_0, l(H, l(H_0, e)))).\n",
    "    * We then consider the rule join(l(H, T), X, l(H, Y)) :- join(T, X, Y), change it to join(l(H_1, T), X_1, l(H_1, Y_0)) :- join(T, X_1, Y_0), unifies join(T_0, l(H, l(H_0, T_0)), Y_1) and join(l(H_1, T), X_1, l(H_1, Y_0)) with the equalities T_0 = l(H_1, T), X_1 = l(H, l(H_0, T_0)) and Y_1 = l(H_1, Y_0), from which we derive T_0 = l(H_1, T), X_1 = l(H, l(H_0, l(H_1, T))), Y_1 = l(H_1, Y_0), X = l(H, l(H_0, l(H_1, T))) and Y = l(H, l(H_0, l(H_1, Y_0))), having to now solve the goal join(T, l(H, l(H_0, l(H_1, T))), Y_0).\n",
    "      * ...\n",
    "\n",
    "The search tree is infinite and looks as follows:\n",
    "\n",
    "![](tree_3.pdf)\n",
    "\n",
    "Observe that if the clauses that define the join function were listed in reverse order, that is, as\n",
    "\n",
    "join(l(H, T), X, l(H, Y)) :- join(T, X, Y).  \n",
    "join(e, X, X).\n",
    "\n",
    "then the search tree would look as follows:\n",
    "\n",
    "![](tree_4.pdf)\n",
    "\n",
    "So solving goals relative to a given definite logic program amounts to exploring a tree of the kind illustrated above; in fact, it is discovering, building the tree, and reporting solutions when reaching a _leaf_ associated with a solution. Two fundamental ways of exploring, or discovering, or building a tree are:\n",
    "\n",
    "* __Depth-first__: the leftmost unexplored _branch_ is explored all the way down, _backtracking_ up to the first embranchment where new branches have still not been explored.\n",
    "* __Breadth-first__: the _nodes_ are visited _level_ by level, from left to right on a given level.\n",
    "\n",
    "To illustrate and experiment, let us consider the tree below:\n",
    "\n",
    "![](tree_5.pdf)\n",
    "\n",
    "The `defaultdict` class from the `collections` module offers an elegant way to represent a tree. The code in the following cell works as follows.\n",
    "\n",
    "* `t = tree()` makes `t` denote a `defaultdict` object, say $t$.\n",
    "* `t['A']['B']['F'] = None` executes as follows. An attempt is made to access the key `'A'` of $t$, which does not exist and is therefore created, with as value what `tree()`, returns, namely, a new `defaultdict` object, say $t_1$. So `t['A']` evaluates to $t_1$. An attempt is made to access the key `'B'` of $t_1$, which does not exist and is therefore created, with as value what `tree()`, returns, namely, a new `defaultdict` object, say $t_2$. So `t['A']['B']` evaluates to $t_2$. An attempt is made to access the key `'F'` of $t_2$, which does not exist and is therefore created, with as value what `tree()`, returns, namely, a new `defaultdict` object, say $t_3$. So `t['A']['B']['C']` evaluates to $t_3$ and is then changed to `None`.\n",
    "* `t['A']['C']['G'] = None` executes as follows. An attempt is made to access the key `'A'` of $t$, which exists and  with `t['A']` evaluating to $t_1$. An attempt is made to access the key `'C'` of $t_1$, which does not exist and is therefore created and becomes the second key of $t_1$, with as value what `tree()`, returns, namely, a new `defaultdict` object, say $t_4$. So `t['A']['C']` evaluates to $t_4$. An attempt is made to access the key `'G'` of $t_4$, which does not exist and is therefore created, with as value what `tree()`, returns, namely, a new `defaultdict` object, say $t_5$. So `t['A']['C']['G']` evaluates to $t_5$ and is then changed to `None`.\n",
    "* ...\n",
    "\n",
    "The `pprint()` function from the `pprint` module makes it easier to see that `t` indeeds models the tree as intended:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree():\n",
    "    return defaultdict(tree)\n",
    "\n",
    "t = tree()\n",
    "t['A']['B']['F'] = None\n",
    "t['A']['C']['G'] = None\n",
    "t['A']['C']['H']['K'] = None\n",
    "t['A']['C']['H']['L']['R'] = None\n",
    "t['A']['C']['H']['M'] = None\n",
    "t['A']['D'] = None\n",
    "t['A']['E']['I']['N'] = None\n",
    "t['A']['E']['I']['O']['S'] = None\n",
    "t['A']['E']['J']['P']['T'] = None\n",
    "t['A']['E']['J']['Q'] = None\n",
    "\n",
    "pprint(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this approach, a tree $t$ is modeled as a dictionary with a unique key, namely, the label of $t$'s __root__ (with `t` above as example, `'A'`), with as associated value, a dictionary that has as many keys as $t$'s root has __children__. That dictionary can be thought of as modeling a __forest__, namely, the collection of each __subtree__ of $t$ that has as root a child of $t$'s root (with `t` above as example, the subtrees rooted at `'B'`, `'C'`, `'D'` and `'E'`).\n",
    "\n",
    "A recursive function makes it easy to explore a tree and list all nodes in a depth-first manner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursively_list_nodes_depth_first(t):\n",
    "    if t is None:\n",
    "        return\n",
    "    for node in t:\n",
    "        print(node, end=' ')\n",
    "        recursively_list_nodes_depth_first(t[node])\n",
    "        \n",
    "recursively_list_nodes_depth_first(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is easy to list the __paths__ from the root of the tree in a depth-first manner, so output [A], [A, B], [A, B, F], [A, C], [A, C, G], [A, C, H]... rather than A, B, F, C, G, H..., with the help of a __stack__. It is easy to list either the nodes or the paths from the root of the tree in a breadth-first manner with the help of a __queue__. Stacks are queues are essentially lists with a limited set of methods:\n",
    "\n",
    "* For stacks, elements can be added and removed at one end (as plates brought to and removed from the top of a stack of plates, the sequence being viewed vertically rather than horizontally, with the end where the action takes place at the top). \n",
    "* For queues, elements can be added at one end and removed at the other end (as individuals queueing at a bus stop,   joining the queue at its back and leaving it, boarding the bus, at its front).\n",
    "\n",
    "    Python lists with their `append()` and `pop()` methods offer suitable implementations of stacks, as the time complexity of both operations is constant in amortised cost. On the other hand, Python lists do not offer an effective implementation of queues: removing the first element of a list and inserting an element at the beginning of a list both have time complexity that is linear in the length of the list. The `deque` class from the `collections` module combines the functionality of stacks and queues, as it has methods for adding and removing elements at boths end that all have constant time complexity (thanks to a data structure known as a __doubly linked list__). Let us first use a `dequeue` object as a stack:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively: stack = deque([])\n",
    "stack = deque(); stack\n",
    "stack.append(0); stack\n",
    "stack.append(1); stack\n",
    "stack.append(2); stack\n",
    "stack.pop(); stack # Two outputs\n",
    "stack.append(3); stack\n",
    "stack.append(4); stack\n",
    "stack.pop(); stack # Two outputs\n",
    "stack.pop(); stack # Two outputs\n",
    "stack.pop(); stack # Two outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can let a `deque` object $O$ model a queue in two ways, depending on how we match the ends of the queue with the ends of $O$. We can let the end of $O$ correspond to the front of the queue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively: queue = deque([])\n",
    "queue = deque(); queue\n",
    "queue.appendleft(0); queue\n",
    "queue.appendleft(-1); queue\n",
    "queue.appendleft(-2); queue\n",
    "queue.pop(); queue # Two outputs\n",
    "queue.appendleft(-3); queue\n",
    "queue.appendleft(-4); queue\n",
    "queue.pop(); queue # Two outputs\n",
    "queue.pop(); queue # Two outputs\n",
    "queue.pop(); queue # Two outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can let the end of the `deque` object correspond to the back of the queue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively: queue = deque([])\n",
    "queue = deque(); queue\n",
    "queue.append(0); queue\n",
    "queue.append(1); queue\n",
    "queue.append(2); queue\n",
    "queue.popleft(); queue # Two outputs\n",
    "queue.append(3); queue\n",
    "queue.append(4); queue\n",
    "queue.popleft(); queue # Two outputs\n",
    "queue.popleft(); queue # Two outputs\n",
    "queue.popleft(); queue # Two outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than appending elements to the back of a queue one after the other, we can prefer to in one sweep move, extend the back of the queue with all elements to append. In case the end of a `deque` object corresponds to the back of the queue it models, then the `extend()` method expectedly does the job. But in case the beginning of a `deque` object corresponds to the back of the queue, then the `extendleft()` method appropriately does the job too. Note how in the following cell, intending to append -1, then -2, then -3 on the left hand side, -3 indeed becomes the leftmost element, with -2 to its right, and -1 to the right of -2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queue = deque([0])\n",
    "queue.extend([1, 2, 3])\n",
    "queue.extendleft([-1, -2, -3])\n",
    "queue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing a recursive function that explores a tree and lists all nodes in a depth-first manner was easy because behind the scene, a stack manages all recursive calls (more generally, a stack manages all function calls). Let us now perform that exploration without taking advantage of recursion; instead, let us explicitly use a stack. Considering the dictionary `t` that models the tree $t$ as defined above, `iter(t)` creates an iterable that can yield all of `t`'s keys. Actually, `t` has only one key, namely, the label of $t$'s root (`'A'`), which can be generated with `next(iter(t))`; let `root` denote it. Then `t[root]` is a dictionary that has as many keys as $t$'s root has children (namely 4, labeled `'B'`, `'C'`, `'D'` and `'E'`). Since the keys of `t[root]` have been inserted into the dictionary starting with the label of the leftmost child of $t$'s root (`'B'`), and proceeding from left to right all the way to the label of the rightmost child of $t$'s root (`'E'`), `reversed(list(t[root]))` is an iterable that can yield those labels starting with the rightmost child of $t$'s root and ending with the leftmost child of $t$'s root (so in the order `'E'`, `'D'`, `'C'` and `'B'`). Adding to the top of a stack `(k, t[root][k])` with `k` yielded by `reversed(list(t[root]))`, we eventually get in the stack a pair of the form $(l_1, f_1)$ where $l_1$ is the label (`'E'`) of the rightmost child of $t$'s root and $f_1$ is the forest consisting of the trees rooted at that node (so the trees rooted at the nodes labeled `'I'` and `'J'`), and above in the stack a pair of the form $(l_2, f_2)$ where $l_2$ is the label (`'D'`) of the second rightmost child of $t$'s root and $f_2$ is an empty forest since that child has no child, and above in the stack a pair of the form $(l_3, f_3)$ where $l_3$ is the label (`'C'`) of the third rightmost child of $t$'s root and $f_3$ is the forest consisting of the trees rooted at that node (so the trees rooted at the nodes labeled `'G'` and `'H'`), and at the top of the stack a pair of the form $(l_4, f_4)$ where $l_4$ is the label (`'B'`) of the leftmost child of $t$'s root and $f_4$ is the forest consisting of the trees rooted at that node (so only one tree, rooted at the node labeled `'F'`). The last pair is indeed the pair that we want to pop first from the stack, since when exploring $t$ in a depth-first manner, the nodes in $f_4$ are enumerated before the nodes in $f_3$, that are enumerated before the nodes in $f_2$, that are enumerated before the nodes in $f_1$. The next cell implements a function that explores a tree depth-first search and calls it with `t` passed as argument. The cell is followed with a cell that traces execution of that function call, replacing the forests stored in the stack with the roots of their trees:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_nodes_depth_first(t):\n",
    "    root = next(iter(t))\n",
    "    roots_and_forests = deque([(root, t[root])])\n",
    "    while roots_and_forests:\n",
    "        root, forest = roots_and_forests.pop()\n",
    "        print(root, end=' ')\n",
    "        if forest:\n",
    "            roots_and_forests.extend((k, forest[k])\n",
    "                                          for k in reversed(list(forest))\n",
    "                                    )\n",
    "\n",
    "list_nodes_depth_first(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = next(iter(t))\n",
    "roots_and_forests = deque([(root, t[root])])\n",
    "print('Stack now (with trees changed to roots):\\n     ',\n",
    "      [(root, [k for k in t[root]])]\n",
    "     )\n",
    "while roots_and_forests:\n",
    "    root, forest = roots_and_forests.pop()\n",
    "    print()\n",
    "    if forest:\n",
    "        print('Node output:', root, '\\t\\tRoots of forest to process:',\n",
    "              [k for k in forest]\n",
    "             )\n",
    "    else:\n",
    "        print('Node output:', root, '\\t\\tEmpty forest')\n",
    "    if forest:\n",
    "        roots_and_forests.extend((k, forest[k])\n",
    "                                      for k in reversed(list(forest))\n",
    "                                )\n",
    "    print('Stack now (with trees changed to roots):\\n     ',\n",
    "          [(root, [k for k in forest] if forest else None)\n",
    "                 for (root, forest) in roots_and_forests\n",
    "          ]\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is easy to modify the previous function to enumerate paths from the root of the tree rather than nodes: instead of storing nodes, we store paths, starting with the path that starts from and stops at the root of the tree (`['A']`) and creating extensions of a path $p$ with each of the children of the last node in $p$, unless that node is a leaf:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_paths_depth_first(t):\n",
    "    root = next(iter(t))\n",
    "    paths_and_forests = deque([([root], t[root])])\n",
    "    while paths_and_forests:\n",
    "        path, forest = paths_and_forests.pop()\n",
    "        print(path)\n",
    "        if forest:\n",
    "            paths_and_forests.extend((path + [k], forest[k])\n",
    "                                          for k in reversed(list(forest))\n",
    "                                    )\n",
    "        \n",
    "list_paths_depth_first(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To explore a tree in a breadth-first manner and generate either nodes or paths, it suffices to modify the previous two functions, using a queue rather than a stack. Also, the left to right ordering of children of a given node should not be reversed. More precisely, the comments for `list_nodes_depth_first()` can be modified as follows (with `t` still denoting the dictionary that models the tree $t$ as defined above and `root` still denoting `next(iter(t))`, that is, the label of $t$'s root (`'A'`)). Adding to the back of a queue `(k, t[root][k])` with `k` yielded by `iter(t[root])`, we eventually get in the queue a pair of the form $(l_1, f_1)$ where $l_1$ is the label (`'B'`) of the leftmost child of $t$'s root and $f_1$ is the forest consisting of the trees rooted at that node (so only one tree, rooted at the node labeled `'F'`), and before in the queue a pair of the form $(l_2, f_2)$ where $l_2$ is the label (`'C'`) of the second lefttmost child of $t$'s root and $f_2$ is the forest consisting of the trees rooted at that node (so the trees rooted at the nodes labeled `'G'` and `'H'`), and before in the queue a pair of the form $(l_3, f_3)$ where $l_3$ is the label (`'D'`) of the third leftmost child of $t$'s root and $f_3$ is an empty forest since that child has no child, and at the end of the queue a pair of the form $(l_4, f_4)$ where $l_4$ is the label (`'E'`) of the rightmost child of $t$'s root and $f_4$ is the forest consisting of the trees rooted at that node (so the trees rooted at the nodes labeled `'I'` and `'J'`). The first pair is indeed the pair that we want to come to the front of the queue and be removed before all others, since when exploring $t$ in a breadth-first manner, the nodes on a given level should be enumerated from left to right. The function `list_nodes_depth_first()` is modified into the function `list_nodes_breadth_first()` in the next cell, in which the function is then called with `t` passed as argument. The cell is followed with a cell that traces execution of that function call, replacing the forests stored in the queue with the roots of their trees:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_nodes_breadth_first(t):\n",
    "    root = next(iter(t))\n",
    "    roots_and_forests = deque([(root, t[root])])\n",
    "    while roots_and_forests:\n",
    "        root, forest = roots_and_forests.pop()\n",
    "        print(root, end=' ')\n",
    "        if forest:\n",
    "            roots_and_forests.extendleft(forest.items())\n",
    "\n",
    "list_nodes_breadth_first(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = next(iter(t))\n",
    "roots_and_forests = deque([(root, t[root])])\n",
    "print('Queue now (with trees changed to roots):\\n     ',\n",
    "      [(root, [k for k in t[root]])]\n",
    "     )\n",
    "while roots_and_forests:\n",
    "    root, forest = roots_and_forests.pop()\n",
    "    print()\n",
    "    if forest:\n",
    "        print('Node output:', root, '\\t\\tRoots of forest to process:',\n",
    "              [k for k in forest]\n",
    "             )\n",
    "    else:\n",
    "        print('Node output:', root, '\\t\\tEmpty forest')\n",
    "    if forest:\n",
    "        roots_and_forests.extendleft(forest.items())\n",
    "    print('Queue now (with trees changed to roots):\\n     ',\n",
    "          [(root, [k for k in forest] if forest else None)\n",
    "                 for (root, forest) in roots_and_forests\n",
    "          ]\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`list_paths_depth_first()` is modified into `list_paths_breadth_first()` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_paths_breadth_first(t):\n",
    "    root = next(iter(t))\n",
    "    paths_and_forests = deque([([root], t[root])])\n",
    "    while paths_and_forests:\n",
    "        path, forest = paths_and_forests.pop()\n",
    "        print(path)\n",
    "        if forest:\n",
    "            paths_and_forests.extendleft((path + [k], forest[k])\n",
    "                                              for k in forest\n",
    "                                        )\n",
    "        \n",
    "list_paths_breadth_first(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting back to both trees for the goal grandparent(john, X):\n",
    "\n",
    "* Exploring the first tree in a depth-first manner yields both solutions fastest, after which the rest of the exploration is \"for nothing\".\n",
    "* Exploring the second tree in a depth-first manner yields both solutions at the very end.\n",
    "* Exploring the first and second trees in a breadth-first manner yields both solutions at the very end.\n",
    "\n",
    "Getting back to both trees for the goal join(X, X, Y):\n",
    "\n",
    "* Exploring the first tree in a depth-first or breadth-first manner makes no difference; the solutions are generated one by one, and the exploration would have to be interrupted at some point as it could go on forever and produce infinitely many solutions.\n",
    "* Exploring the second tree in a breadth-first manner is hardly different to exploring the first tree in a breadth-first manner, one node being visited before rather than after the production of a given solution.\n",
    "* Exploring the second tree in a depth-first manner traps the search in an infinite descent along the leftmost branch, with no solution being ever produced.\n",
    "\n",
    "Solving goals relative to a given definite logic program by a breadth-first exploration of the associated search tree is a __complete__ proof procedure: every solution is eventually produced. This is an immediate consequence of the fact that such a tree is __finitely branching__. On the other hand, as we have just observed, a depth-first exploration does not offer a complete proof procedure. Usually, Prolog's proof engine implements a depth-first search, hence an incomplete proof procedure: it expects users to properly order the rules that make up the program, and to properly order in a rule the atoms that make up its body, so that the search trees associated with goals of interest have \"a good shape\" and make depth-first search a most efficient procedure. We will write a Prolog interpreter where we can chose to explore a search tree either in a depth-first or in a breadth-first manner (the difference is minor and essentially relies on using either a stack or a queue in pretty much the same way, as we have previously observed). The exploration of a search tree is the last part of the work the interpreter has to do. First, we need to be able to parse the rules that make up a definite logic program, as well as perform a number of operations such as separate head and body from a rule, identify which variables occur in an atom, perform substitution of variables by terms in an atom, etc. We first define a couple of helper functions to consistently extend a dictionary with a new key-value pair, or all the key-value pairs from another dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consistently_add_to(data_pair, mapping):\n",
    "    return mapping.setdefault(data_pair[0], data_pair[1]) == data_pair[1]\n",
    "\n",
    "mapping = {'A': 1, 'B': 2, 'C': 3}\n",
    "consistently_add_to(('B', 4), mapping), mapping\n",
    "consistently_add_to(('B', 2), mapping), mapping\n",
    "consistently_add_to(('D', 4), mapping), mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consistently_merge_to(mapping_1, mapping_2):\n",
    "    for data_pair in mapping_1.items():\n",
    "        if not consistently_add_to(data_pair, mapping_2):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "mapping = {'A': 1, 'B': 2, 'C': 3}\n",
    "consistently_merge_to({'B': 2, 'C': 4, 'D': 5}, mapping), mapping\n",
    "consistently_merge_to({'B': 2, 'C': 3}, mapping), mapping\n",
    "consistently_merge_to({'C': 3, 'D': 4, 'E': 5}, mapping), mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define three classes, `Expression`, `Term` and `Atom`, with `Term` and `Atom` inheriting from `Expression`, considering that terms and atoms are two kinds of expressions.\n",
    "\n",
    "Generalising on the examples previously examined:\n",
    "\n",
    "* an atom is built from an $n$-ary predicate symbol ($n\\in\\mathbf N$) and $n$ terms;\n",
    "* a term is a variable or is built from an $n$-ary function symbol ($n\\in\\mathbf N$) and $n$ terms.\n",
    "\n",
    "So the outermost symbol in an atom is a predicate symbol while all other symbols are function symbols or variables; all symbols in a term are function symbols or variables. Prolog function symbols, predicate symbols and variables are built from alphanumeric characters and underscores, with function and predicate symbols starting with lowercase letters, and variables starting with uppercase letters or underscores.\n",
    "\n",
    "Trees are most appropriate to represent expressions. To give `Expression` objects the structure of trees that represent expressions, we use two attributes, `root` and `children`. Given an object $O$ of classs `Expression` meant to represent an expression $E$:\n",
    "\n",
    "* the value of `root` for $O$ will be set to the string that denotes $E$'s outermost symbol $s$ (a predicate symbol if $E$ is an atom, a function symbol if $E$ is a term);\n",
    "* if $n$ is $s$'s arity, the value of `children` for $O$ will be set to a list of length $n$ consisting of $n$ `Expression` objects, that represent the $n$ terms that are the arguments to $s$ in $E$, listed from left to right (so the list is empty in case $E$ is a nullary predicate symbol, a constant, or a variable).\n",
    "\n",
    "`Expression` objects will be created by parsing strings, giving atoms and terms a tree structure. We implement `__str__()` in `Expression` for what can be seen as the reverse operation: get the (properly formatted) string from the representing tree. When testing `__str__()` below, we define `Expression` objects \"by hand\"; parsing methods will be defined next:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Expression:\n",
    "    def __init__(self, root, children=None):\n",
    "        self.root = root\n",
    "        if children is None:\n",
    "            children = []\n",
    "        self.children = children\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.root if not self.children\\\n",
    "                    else ''.join((self.root, '(',\n",
    "                                 ', '.join(child.__str__()\n",
    "                                                for child in self.children\n",
    "                                          ), ')'\n",
    "                                 )\n",
    "                                )\n",
    "\n",
    "\n",
    "class Term(Expression):\n",
    "    class TermError(Exception):\n",
    "        pass\n",
    "\n",
    "    def __init__(self, root, children=None):\n",
    "        super().__init__(root, children)  \n",
    "\n",
    "        \n",
    "class Atom(Expression):\n",
    "    class AtomError(Exception):\n",
    "        pass\n",
    "\n",
    "    def __init__(self, root, children=None):\n",
    "        super().__init__(root, children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bob: constant\n",
    "print(Term('bob'))\n",
    "# l: binary function symbol\n",
    "# H, T: variables\n",
    "print(Term('l', [Term('H'), Term('T')]))\n",
    "# l: binary function symbol\n",
    "# e: constant\n",
    "# H, T: variables\n",
    "print(Term('l', [Term('e'), Term('l', [Term('H'), Term('T')])]))\n",
    "\n",
    "print()\n",
    "\n",
    "# on: nullary predicate symbol\n",
    "print(Atom('on'))\n",
    "# happy: unary predicate symbol\n",
    "# john: constant\n",
    "print(Atom('happy', [Term('john')]))\n",
    "# mother: binary predicate symbol\n",
    "# jane, sandra: constants\n",
    "print(Atom('mother', [Term('jane'), Term('sandra')]))\n",
    "# join: binary predicate symbol\n",
    "# l: binary function symbol\n",
    "# H, T, X, Y: variables\n",
    "print(Atom('join', [Term('l', [Term('H'), Term('T')]), Term('X'),\n",
    "                    Term('l', [Term('H'), Term('Y')])\n",
    "                   ]\n",
    "          )\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To parse an atom or a term represented as a string, we will first get rid of all spaces in the string, if any, and convert the resulting string to a list $L$ of characters, from last character in the string to first character in the string, so that characters can be efficiently consumed by popping them off the end of the list, as opposed to removing them from the beginning of the list. Opening and closing parentheses and commas will need special processing. The rest is predicate and function symbols and variables, which will be dealt with thanks to the function `parse_word()` of the `Expression` class. This function receives as argument what remains of $L$, assumed to be at the stage where a predicate or function symbol or a variable is to be parsed; so $L$ then ends in the characters that make up that predicate or function symbol or variable in reverse order; `parse_word()` consumes those characters from $L$ and returns the predicate or function symbol or the variable as a string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Expression(Expression):\n",
    "    def parse_word(characters):\n",
    "        word = [characters.pop()]\n",
    "        while characters and (characters[-1].isalnum()\n",
    "                              or characters[-1] == '_'\n",
    "                             ):\n",
    "            word.append(characters.pop())\n",
    "        return ''.join(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "characters = list(reversed('bob'.replace(' ', '')))\n",
    "characters\n",
    "Expression.parse_word(characters)\n",
    "characters\n",
    "\n",
    "print()\n",
    "\n",
    "characters = list(reversed('happy(  john  )'.replace(' ', '')))\n",
    "characters\n",
    "Expression.parse_word(characters)\n",
    "characters\n",
    "\n",
    "print()\n",
    "\n",
    "# Could be what remains to be parsed in \"join(l(H, T), X, l(H, Y))\"\n",
    "# after \"join(l(H, T), X, l(\" has been parsed already.\n",
    "characters = list(reversed('H,  Y))'.replace(' ', '')))\n",
    "characters\n",
    "Expression.parse_word(characters)\n",
    "characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let an atom or a term $E$ be given and let $L$ be the list of all nonspace characters in $E$ in reverse order. We will define in `Expression` two functions, `parse_subitem()` and `parse_subitem_sequence()`, meant to operate in a way that we now describe. To parse $E$ (possibly as a subexpression of a larger expression), the function `parse_subitem()` will be called with $L$ passed as argument. It will first check that $E$ indeed starts with (that is, $L$ indeed ends in) a character that can be the beginning of a predicate or function symbol or a variable and call `parse_word()`, passing $L$ as argument. If $E$ is a nullary predicate or function symbol or a variable, that predicate or function symbol or variable will be returned by `parse_word()` and $L$ will have become empty. Otherwise, $E$ is of the form $\\sigma(t_1,\\dots,t_n)$ for some nonzero $n\\in\\mathbf N$, predicate or function symbol $\\sigma$, and terms $t_1$, ..., $t_n$. Then `parse_word()` will return $\\sigma$, having consumed all characters that make up $\\sigma$; `parse_subitem()`, finding out that $L$ is not empty, will then check that $L$ indeeds ends in `(`, consume that character (that it, pop `(` off the end of $L$), and call `parse_subitem_sequence()`, passing $L$ as argument, whose purpose is to parse $t_1,\\dots, t_n$ and return a list with as members, the $n$ `Term` objects $o_1$, ..., $o_n$ that represent $t_1$, ..., $t_n$. At this stage, all characters occurring in $t_1$, ..., $t_n$ and the separating commas will have been consumed, and the only task left for `parse_subitem()` to complete will be to check that `)` is now the last symbol in $L$ (it should also be the only symbol in $L$ in case $E$ is the whole expression to parse, but that will not be up to `parse_subitem()` to check: `parse_subitem()` does not know whether it parses a whole expression or a subexpression of a larger expression), consume it, and create an `Atom` or a `Term` object whose `root` attribute should be set to $\\sigma$ and whose `children` attribute should be set to the list $[o_1,\\dots,o_n]$. Recall that `parse_subitem_sequence()` will have to parse $t_1,\\dots, t_n$. It will do so with a first call to `parse_subitem()` to create a `Term` object $o_1$ that represents $t_1$. It will then find out that $L$ now ends in a comma, consume it, and make a second call to `parse_subitem()` to create a `Term` object $o_2$ that represents $t_2$. Eventually, it will make a last call to `parse_subitem()` to create a `Term` object $o_n$ that represents $t_n$, find out that $L$ does not end in a comma, assume that the whole sequence has been successfully parsed, and return $[o_1,\\dots,o_n]$ to its caller (the original `parse_subitem()` call).\n",
    "\n",
    "Before we implement `parse_subitem()` and `parse_subitem_sequence()`, we define skeleton functions to illustrate how `parse_subitem()` and `parse_subitem_sequence()` are meant to call each other, and how characters are consumed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_subitem_skeleton(characters, depth=0):\n",
    "    print('  ' * depth, 'Start parsing subitem, characters left:',\n",
    "          ''.join(reversed(characters))\n",
    "         )\n",
    "    Expression.parse_word(characters)\n",
    "    if not characters or characters[-1] != '(':\n",
    "        print('  ' * depth, 'End parsing subitem, characters left:',\n",
    "              ''.join(reversed(characters))\n",
    "             )\n",
    "        return\n",
    "    # Popping (\n",
    "    characters.pop()\n",
    "    parse_subitem_sequence_skeleton(characters, depth + 1)\n",
    "    # Popping )\n",
    "    characters.pop()\n",
    "    print('  ' * depth, 'End parsing subitem, characters left:',\n",
    "          ''.join(reversed(characters))\n",
    "         )\n",
    "\n",
    "def parse_subitem_sequence_skeleton(characters, depth):\n",
    "    print('  ' * depth, 'Start parsing subitem sequence, characters left:',\n",
    "          ''.join(reversed(characters))\n",
    "         )\n",
    "    expressions = []\n",
    "    while True:\n",
    "        expressions.append(parse_subitem_skeleton(characters, depth + 1))\n",
    "        if characters[-1] != ',':\n",
    "            print('  ' * depth,\n",
    "                  'End parsing subitem sequence, characters left:',\n",
    "                  ''.join(reversed(characters))\n",
    "                 )\n",
    "            return\n",
    "        # Popping ,\n",
    "        characters.pop()\n",
    "        \n",
    "parse_subitem_skeleton(list(reversed('bob')))\n",
    "print()\n",
    "parse_subitem_skeleton(list(reversed('happy(john)')))\n",
    "print()\n",
    "parse_subitem_skeleton(list(reversed('l(e,l(H,T))')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the implementation of `parse_subitem()` and `parse_subitem_sequence()`. Besides the list of characters $L$, both have a second parameter, `item_type`, meant to be set to a string that represents the type of expression to parse, namely, either `Atom` or `Term`. At this stage of the discussion, `parse_subitem()` seems to be useful to parse either atoms or terms, whereas `parse_subitem_sequence()` seems to be useful only to parse sequences of terms. But to parse the bodies of the rules of a logic program, we will have to parse sequence of atoms; `parse_subitem_sequence()` will be perfectly suitable for the task, with the second parameter of `parse_subitem_sequence()` set to `'Atom'`. Still, as `parse_subitem_sequence()` will be more often used to parse sequences of terms, we give its parameter `item_type` the default value of `'Term'`, which allows `parse_subitem()` to call `parse_subitem_sequence()` with no other argument but $L$.\n",
    "\n",
    "It was tacitly assumed that `parse_subitem_skeleton()` and `parse_subitem_sequence_skeleton()` would be used only to parse a syntactically correct expression; `parse_subitem()` and `parse_subitem_sequence()` make no such assumption, so also check for syntactic correctness, and return `None` whenever they find out that there are characters that cannot be for a syntactically correct expression:\n",
    "\n",
    "* Before it calls `parse_word()`, `parse_subitem()` checks that there is indeed at least one character to parse, and that:\n",
    "  * in case the expression being parsed is an atom, `parse_word()` should return a predicate symbol, hence the first character to parse should be a lowercase letter;\n",
    "  * in case the expression being parsed is a term, `parse_word()` should return a function symbol or a variable, hence the first character to parse should be a letter or an underscore (which is equivalent to `isidentifier()` identifying the string consisting of that single character as an identifier).\n",
    "* In case `parse_subitem()` does not process a nullary predicate or function symbol or a variable, because it finds an opening parenthesis and calls `parse_subitem_sequence()`, and provided that the latter successfully parses a sequence of expressions and does not return `None` (otherwise, `parse_subitem()` should itself return `None`), it needs to finally check that there are some characters left, with as first character to parse, a closing parenthesis.\n",
    "* As for `parse_subitem_sequence()`, provided that all calls to `parse_subitem()` successfully parse an expression and do not return `None` (otherwise, `parse_subitem_sequence()` should itself return `None`), it has to verify that there is at least one character left before checking whether that character is a comma, indicating that furher expressions still have to be parsed in the sequence being currently dealt with; if that is not the case, `parse_subitem_sequence()` does not have to return `None`: `parse_subitem()` will, as caller to `parse_subitem_sequence()`, return `None`.\n",
    "\n",
    "`parse_subitem()` creates `Atom` or `Term` objects (the objects in the sequence of objects created by `parse_subitem_sequence()` are created by `parse_subitem()`); the `item_type` parameter informs `parse_subitem()` of the appropriate type. The `globals()` function returns a dictionary with `'Atom'` and `'Term'` as particular keys, with as corresponding values, the types themselves.\n",
    "\n",
    "`parse_subitem()` and `parse_subitem_sequence()` are both meant to be used as helper functions. We extend the `Expression` class with a function, `parse_item()`, that:\n",
    "\n",
    "* gets from a string $\\sigma$ a list of characters as previously described (nonspace characters in $\\sigma$ removed, and listed from last to first);\n",
    "* calls by default `parse_subitem()`, because the `parse_single_subitem` parameter of `parse_item()` evaluates to `True`, the default value, which is suitable for the case where $\\sigma$ represents an atom or a term, but as pointed out above, there will be a need to parse rule bodies, that is, sequences of atoms, and then `parse_single_subitem` will be set to `False` to let `parse_item()` call `parse_subitem_sequence()` instead of `parse_subitem()`;\n",
    "* passes as argument to `parse_subitem()` or `parse_subitem_sequence()` the type of object or objects to be created, thanks to the `item_type` parameter of `parse_item()`, set to `Atom` by default;\n",
    "* is meant to fully parse the string provided as first argument to `parse_item()`, because that argument is not meant to be a subexpression of a larger expression, hence returns `None` in case some characters still remain after `parse_subitem()` or `parse_subitem_sequence()` have returned an object or a sequence of objects (also returning `None` otherwise):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Expression(Expression):\n",
    "    def parse_subitem(characters, item_type):\n",
    "        if not characters or not {'Atom': lambda c: c.islower(),\n",
    "                                  'Term': lambda c: c.isidentifier()\n",
    "                                 }[item_type](characters[-1]):\n",
    "            return\n",
    "        symbol = Expression.parse_word(characters)\n",
    "        if not characters or characters[-1] != '(':\n",
    "            return globals()[item_type](symbol)\n",
    "        characters.pop()\n",
    "        subitems = Expression.parse_subitem_sequence(characters)\n",
    "        if not subitems or not characters or characters.pop() != ')':\n",
    "            return\n",
    "        return globals()[item_type](symbol, subitems)\n",
    "\n",
    "    def parse_subitem_sequence(characters, item_type='Term'):\n",
    "        expressions = []\n",
    "        while True:\n",
    "            expression = Expression.parse_subitem(characters, item_type)\n",
    "            if not expression:\n",
    "                return\n",
    "            expressions.append(expression)\n",
    "            if not characters or characters[-1] != ',':\n",
    "                return expressions\n",
    "            characters.pop()\n",
    "            \n",
    "    def parse_item(expression, item_type='Atom', parse_single_subitem=True):\n",
    "        characters = list(reversed(expression.replace(' ', '')))\n",
    "        item = Expression.parse_subitem(characters, item_type)\\\n",
    "                  if parse_single_subitem\\\n",
    "                  else Expression.parse_subitem_sequence(characters, item_type)\n",
    "        if not item or characters:\n",
    "            return\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expression = Expression.parse_item('bob', 'Term')\n",
    "print(type(expression), expression)\n",
    "\n",
    "expression = Expression.parse_item('l(e,l(H,T))', 'Term')\n",
    "print(type(expression), expression)\n",
    "\n",
    "expression = Expression.parse_item('father(bob,sandra)')\n",
    "print(type(expression), expression)\n",
    "\n",
    "expression = Expression.parse_item('join(l(H,T),X,l(H,Y))')\n",
    "print(type(expression), expression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An atom or term can be successfully parsed while still being an invalid Prolog expression because a predicate or function symbol is used with different arities, or because a symbol is used both as predicate and function symbols. We define in `Expression` a function, `collected_symbols()`, meant to collect the symbols (not variables) that occur in an `Expression` object, in the form of a dictionary, with symbols as keys and symbols' arities as values. We let `collected_symbols()` ignore the outermost symbol of the expression by letting its second parameter, `include_root`, take the default value of `False`. That is suitable for `Expression` objects that are more precisely of type `Atom` since the outermost symbol (the value of their `root` attribute) is a predicate symbol, to be distinguished from the other symbols, all function symbols (besides variables), that occur in the expression that the object represents. On the other hand, for `Expression` objects that are more precisely of type `Term`, setting `include_root` to `True` is appropriate to collect all function symbols, including the outermost one. The lambda expression `top_symbol`, defined in `Expression` and called by `collected_symbols()`, is meant to return the outermost symbol and the arity of an `Expression` object that does not represent a variable. The core of the work is performed recursively thanks to the helper functions `consistently_add_to()` and `consistently_merge_to()`.\n",
    "\n",
    "We extend the `Atom` class with a method `predicate_and_function_symbols()` that collects all function symbols in an object of type `Atom` (provided they are consistently used), and separately, the predicate (outermost) symbol (provided it is different to the function symbols); essentially, the method calls `collected_symbols()`.\n",
    "\n",
    "We extend the `Term` and `Atom` classes with two functions, `parse_term()` and `parse_atom()`, respectively, that essentially call the `parse_item()` function of the `Expression` class, informing it of the kind of expression, hence the type of object, to be created (explicitly for terms, implicitly for atoms). The functions call `collected_symbols()`, with `parse_atom()` performing some extra work, to check that the parsed expression is a correct Prolog term or atom, respectively: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Expression(Expression):\n",
    "    top_symbol = lambda tree: (tree.root, len(tree.children))\\\n",
    "                              if tree.root[0].islower() else None\n",
    "\n",
    "    def collected_symbols(self, include_root=False):\n",
    "        symbols = {}\n",
    "        if include_root:\n",
    "            root = Expression.top_symbol(self)\n",
    "            if root:\n",
    "                symbols[root[0]] = root[1]\n",
    "        for child in self.children:\n",
    "            symbol = Expression.top_symbol(child)\n",
    "            if symbol\\\n",
    "               and not consistently_add_to(symbol, symbols)\\\n",
    "               and not consistently_merge_to(child.collected_symbols(symbol),\n",
    "                                             symbols\n",
    "                                            ):\n",
    "                return\n",
    "        return symbols\n",
    "\n",
    "\n",
    "class Term(Term, Expression):\n",
    "    def parse_term(expression):\n",
    "        term = Expression.parse_item(expression, 'Term')\n",
    "        if not term:\n",
    "            raise Term.TermError(f'{expression}: syntactically incorrect')\n",
    "        if term.collected_symbols(True) is None:\n",
    "            raise Term.TermError('Same function symbol used with '\n",
    "                                 f'different arities in {expression}'\n",
    "                                )\n",
    "        return term\n",
    "\n",
    "\n",
    "class Atom(Atom, Expression):\n",
    "    def predicate_and_function_symbols(self):\n",
    "        return (self.root, len(self.children)), self.collected_symbols()\n",
    "    \n",
    "    def parse_atom(expression):\n",
    "        atom = Expression.parse_item(expression)\n",
    "        if not atom:\n",
    "            raise Atom.AtomError(f'{expression}: syntactically incorrect')\n",
    "        function_symbols = atom.collected_symbols()\n",
    "        if function_symbols is None:\n",
    "            raise Atom.AtomError('Same function symbol used with '\n",
    "                                 f'different arities in {expression}'\n",
    "                                )\n",
    "        if atom.root in function_symbols:\n",
    "            raise Atom.AtomError('Predicate symbol one of function symbols')\n",
    "        return atom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term = Term.parse_term('X')\n",
    "print(term)\n",
    "term.collected_symbols(True)\n",
    "\n",
    "term = Term.parse_term('bob')\n",
    "print(term)\n",
    "term.collected_symbols(True)\n",
    "\n",
    "term = Term.parse_term('l(e, l(H, T))')\n",
    "print(term)\n",
    "term.collected_symbols(True)\n",
    "\n",
    "atom = Atom.parse_atom('father(bob, sandra)')\n",
    "print(atom)\n",
    "atom.predicate_and_function_symbols()\n",
    "\n",
    "atom = Atom.parse_atom('join(l(H, T), X, l(H, Y))')\n",
    "print(atom)\n",
    "atom.predicate_and_function_symbols()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having parsed atoms and terms in the form of `Atom` and `Term` objects, respectively, we can now define a number of functions and methods to operate on `Expression` objects as needed by a Prolog interpreter. It is necessary to be able and find out whether an expression is a variable, and collect all variables that occur in an expression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Expression(Expression):\n",
    "    def is_variable(self):\n",
    "        return self.root[0].isupper() or self.root[0] == '_' \n",
    "\n",
    "    def variables(self):\n",
    "        variables = {self.root} if self.is_variable() else set()\n",
    "        for child in self.children:\n",
    "            variables.update(child.variables())\n",
    "        return variables\n",
    "\n",
    "\n",
    "class Term(Term, Expression):\n",
    "    pass\n",
    "\n",
    "\n",
    "class Atom(Atom, Expression):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Term.parse_term('x').variables()\n",
    "Term.parse_term('X').variables()\n",
    "Term.parse_term('f(X, a, X)').variables()\n",
    "Term.parse_term('f(X_0, Y, X_0)').variables() == {'Y', 'X_0'}\n",
    "Term.parse_term('f(c, f(X, f(a, Z, b), '\n",
    "                'f(f(X, Z, U), a, T)), f(a, U, a))'\n",
    "               ).variables() == {'U', 'T', 'X', 'Z'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prolog considers underscores within a rule as independent variables. For instance, the fact $\\mathit{loves}(\\_,\\_).$ is meant to express that everyone loves everyone (including oneself), not only that everyone loves oneself: the two occurrences of $\\_$ denote arbitrary, independant individuals, and $\\mathit{loves}(\\_,\\_)$ has the same intended meaning as $\\mathit{loves}(\\_0,\\_1)$. A Prolog interpreter needs to treat all occurrences of a given variable in a rule as denotations of the same individual (as a consequence, a Prolog interpreter needs to make sure that whenever an occurrence of a variable in a rule is replaced by a term, then all other occurrences of the variable in the rule are replaced by the term). To that aim, we define in `Expression` a recursive method, `individualise_underscores()`, meant to let in an expression each occurrence of an underscore, used as a full name for a variable, be followed by a unique natural number. We first explain how the method operates thanks to a tracing function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trace_individualise_underscores(expression, variables, index, depth):\n",
    "    print('  ' * depth, f'Received index is {index}, processing', expression)\n",
    "    if expression.root == '_':\n",
    "        while True:\n",
    "            index += 1\n",
    "            next_underscore = '_' + str(index)\n",
    "            if next_underscore not in variables:\n",
    "                expression.root = next_underscore\n",
    "                print('  ' * depth, f'_ changed to {expression}, returned '\n",
    "                      'index is', index\n",
    "                     )\n",
    "                return index\n",
    "    if not expression.children:\n",
    "        print('  ' * depth, 'Returned index is', index)\n",
    "        return index\n",
    "    for child in expression.children:\n",
    "        index = trace_individualise_underscores(child, variables, index,\n",
    "                                                depth + 1\n",
    "                                               )\n",
    "    print('  ' * depth, 'Returned index is', index)\n",
    "    return index\n",
    "    \n",
    "atom = Atom.parse_atom('p(g(h(a, _), X), _2, h(a, _), _)')\n",
    "variables = atom.variables()\n",
    "print('Variables in expression are:', variables, end='\\n\\n')\n",
    "trace_individualise_underscores(atom, variables, -1, 0)\n",
    "print('expression now is:', atom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implementation of `individualise_underscores()` in the following cell is a straightforward adaptation of `trace_individualise_underscores()`. When dealing with a fact, `individualise_underscores()` can be called on the `Atom` object that captures the fact, and the default arguments are appropriate. When dealing with a more general rule, `individualise_underscores()` can be called on the rule's head, but the argument `variables` should be given as value the set $S$ of variables that occur in the whole rule, not just in the rule's head; that call would return an integer $i_1$. Then `individualise_underscores()` can be called on the first atom in the rule's body, with `variables` still set to $S$, but with the argument `index` given the value $i_1$; that call would return an integer $i_2$. Then `individualise_underscores()` can be called on the second atom in the rule's body, if any, with `variables` still set to $S$, but with the argument `index` given the value $i_2$... That is a way to give each underscore that occurs in the rule a unique name, not occurring anywhere in the rule. That is not something to see in action yet as for now, we are only dealing with expressions, not rules and logic programs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Expression(Expression):\n",
    "    def individualise_underscores(self, variables=None, index=-1):\n",
    "        if variables is None:\n",
    "            variables = self.variables()\n",
    "        if self.root == '_':\n",
    "            while True:\n",
    "                index += 1\n",
    "                next_underscore = '_' + str(index)\n",
    "                if next_underscore not in variables:\n",
    "                    self.root = next_underscore\n",
    "                    return index\n",
    "        if not self.children:\n",
    "            return index\n",
    "        for child in self.children:\n",
    "            index = child.individualise_underscores(variables, index)\n",
    "        return index\n",
    "\n",
    "\n",
    "class Term(Term, Expression):\n",
    "    pass\n",
    "\n",
    "\n",
    "class Atom(Atom, Expression):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atom = Atom.parse_atom('p(g(h(a, _), X), _2, h(a, _), _)')\n",
    "atom.individualise_underscores()\n",
    "print(atom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Prolog interpreter needs to be able to substitute in an expression $E$ all occurrences of some of the variables that occur in $E$ by terms; sometimes, the terms will simply be computed fresh variables, for a particular kind of substitution referred to as a _renaming of variables in $E$_. We extend `Expression` with a function, `fresh_variables()`, meant to take two arguments, `variables` and `reserved_variables`, both expected to be sets of variables, and return a dictionary $D$ whose keys are the members of both `variables` and `reserved_variables`, and whose values are pairwise distinct variables, different to those in both sets. The intention is that given an expression $E$, `variables` will denote the set of variables that occur in $E$; any such variable $v$ that happens to belong to `reserved_variables` can then be replaced by $D[v]$ in $E$, resulting in an expression where fresh variables have replaced those in both `variables` and `reserved_variables`. We let the name of a variable that belongs to both `variables` and `reserved_variables` be followed by an underscore and the smallest possible natural number for the mapped to fresh variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Expression(Expression):\n",
    "    def fresh_variables(variables, reserved_variables):\n",
    "        substitutions = {}\n",
    "        # Any variable Var that occurs in both variables and\n",
    "        # reserved_variables will be renamed to Var_i where i is the\n",
    "        # least natural number that makes Var_i a new variable (that is,\n",
    "        # occurring neither in variables nor in reserved_variables nor\n",
    "        # in the set of variables that have been created so far, if\n",
    "        # any).\n",
    "        for variable in variables & reserved_variables:\n",
    "            i = 0\n",
    "            while ''.join((variable, '_', str(i))) in variables\\\n",
    "                                                      | reserved_variables:\n",
    "                i += 1\n",
    "            substitutions[variable] = ''.join((variable, '_', str(i)))\n",
    "        return substitutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Expression.fresh_variables({'a'}, set())\n",
    "Expression.fresh_variables({'Y'}, {'X'})\n",
    "Expression.fresh_variables({'X'}, {'X'})\n",
    "Expression.fresh_variables({'Y', 'Z'}, {'X', 'Y'})\n",
    "Expression.fresh_variables({'U', 'Y', 'Z'}, {'X', 'Y', 'Z'}) ==\\\n",
    "                                           {'Z': 'Z_0', 'Y': 'Y_0'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the Prolog interpreter needs to rename variables in an expression $E$, it  actually always has to leave $E$ untouched and perform the renaming on a copy of $E$. The `deepcopy` function from the `copy` module offers a good solution to cloning an `Expression` object. Compare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = [0]\n",
    "# Alternatively, use the copy method of the List class:\n",
    "# L_copy = L.copy()\n",
    "L_copy = copy(L)\n",
    "L_copy[0] = 1\n",
    "L_copy, L\n",
    "\n",
    "L = [[0]]\n",
    "# Alternatively, use the copy method of the List class:\n",
    "# L_copy = L.copy()\n",
    "L_copy = copy(L)\n",
    "L_copy[0][0] = 1\n",
    "L_copy, L\n",
    "\n",
    "L = [[0]]\n",
    "L_deepcopy = deepcopy(L)\n",
    "L_deepcopy[0][0] = 1\n",
    "L_deepcopy, L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extend `Expression` with a method, `rename_variables()`, that recursively renames variables in a copy of an expression, the renaming taking the form of a dictionary mapping variables to replace to replacing variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Expression(Expression):\n",
    "    def rename_variables(self, substitution):\n",
    "        return deepcopy(self)._rename_variables(substitution)\n",
    "\n",
    "    def _rename_variables(self, substitution):\n",
    "        if self.is_variable():\n",
    "            if self.root in substitution:\n",
    "                self.root = substitution[self.root]\n",
    "        else:\n",
    "            for child in self.children:\n",
    "                child._rename_variables(substitution)\n",
    "        return self\n",
    "\n",
    "\n",
    "class Term(Term, Expression):\n",
    "    pass\n",
    "\n",
    "\n",
    "class Atom(Atom, Expression):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atom = Atom.parse_atom('join(l(H, T),X, l(H, Y))')\n",
    "print(atom.rename_variables({'X': 'A', 'Y': 'B', 'Z': 'C'}))\n",
    "print(atom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More general substitutions of variables by terms in an expression $E$ will have to be performed sometimes in $E$ itself, sometimes in a copy of $E$. We extend `Expression` with two methods, `substitute()` and `substitute_in_copy()`, for both kinds of substitutions, the latter just calling the former on a copy of the object it applies the method to. When a term $t$ replaces a variable $v$ in $E$, $t$ might itself contain variables that belong to the domain of the substitution. In that case, we apply the substitution again, many times if needed. If the substitution requested to replace a variable by itself, or requested to replace a variable $v_1$ by a variable $v_2$ and the other way around, the procedure would loop forever. The kind of substitution that will be used in practice will not make this possible: after a finite number of applications of the substitution, the resulting expression will contain no occurrence of a variable in the domain of the substitution. Note that the assignment in the body of `substitute()` cannot be replaced by `self = substitution[self.root]`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Expression(Expression):\n",
    "    def substitute_in_copy(self, substitution):\n",
    "        return deepcopy(self).substitute(substitution)\n",
    "\n",
    "    def substitute(self, substitution):\n",
    "        if self.is_variable():\n",
    "            if self.root in substitution:\n",
    "                self.root, self.children = substitution[self.root].root,\\\n",
    "                                           substitution[self.root].children\n",
    "                self.substitute(substitution)\n",
    "        else:\n",
    "            for child in self.children:\n",
    "                child.substitute(substitution)\n",
    "        return self\n",
    "\n",
    "\n",
    "class Term(Term, Expression):\n",
    "    pass\n",
    "\n",
    "\n",
    "class Atom(Atom, Expression):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term = Term.parse_term('f(a, X, g(Y, b), h(h(h(Z))))')\n",
    "print(term.substitute({'X': Term.parse_term('X_0'), 'Z': Term.parse_term('Z_0')\n",
    "                      }\n",
    "                     )\n",
    "     )\n",
    "\n",
    "term = Term.parse_term('h(h(f(U, Y, g(Z, Z))))')\n",
    "print(term.substitute({'Z': Term.parse_term('V')}))\n",
    "\n",
    "term = Term.parse_term('f(a, X, g(Y, b), h(h(h(Z))))')\n",
    "print(term.substitute({'X_0' : Term.parse_term('h(Z_0)'),\n",
    "                       'X': Term.parse_term('X_0'), 'Z_0': Term.parse_term('c')\n",
    "                      }\n",
    "                     )\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the heart of Prolog lies the _unification algorithm_. It computes a most general unifier (_mgu_) for two expressions $E_1$ and $E_2$, that is, a substitution $\\theta$ such that:\n",
    "\n",
    "* applying $\\theta$ to $E_1$ and $E_2$ results in the same expression;\n",
    "* for any substitution $\\psi$ that applied to $E_1$ and $E_2$, results in the same expression, there exists a substitution $\\nu$ such that applying $\\psi$ to $E_1$ and $E_2$ is the same as applying $\\theta$ to $E_1$ and $E_2$, and then $\\nu$ to the resulting expressions.\n",
    "\n",
    "For instance, if $E_1$ is $f(X_1, h(X_1), X_2)$ and $E_2$ is $f(g(X_3), X_4, X_3)$, then the substitution that maps $X_1$ to $g(X_3)$, $X_2$ to $X_3$ and $X_4$ to $h(g(X_3))$ is an mgu for $E_1$ and $E_2$; when applied to $E_1$ and $E_2$, it results in the expression $f(g(X_3), h(g(X_3)), h(g(X_3)))$. If $a$ is a constant, then the substitution that maps $X_1$ to $g(a)$, $X_2$ to $a$ and $X_4$ to $h(g(a))$ results in the same expression when applied to $E_1$ and $E_2$, namely, $f(g(a), h(g(a)), h(g(a)))$; but it suffices to apply the substitution that maps $X_3$ to $a$ to get $f(g(a), h(g(a)), h(g(a)))$ from $f(g(X_3), h(g(X_3)), h(g(X_3)))$.\n",
    "\n",
    "So a most general unifier for two expressions $E_1$ and $E_2$ is a substitution $\\theta$ that specialises $E_1$ and $E_2$ to the same expression in the most general way: any other substitution that specialises $E_1$ and $E_2$ to the same expression can be obtained by instantiating $\\theta$. A most general unifier is unique up to a renaming of variables.\n",
    "\n",
    "The following tracing function, `trace_unify_identities()`, is meant to explain and illustrate the unification algorithm. It takes as argument a list of pairs of expressions. To compute an mgu for two expressions $E_1$ and $E_2$, `trace_unify_identities()` is called with $[(E_1,E_2)]$ provided as argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trace_unify_identities(identities):\n",
    "    mgu = {}\n",
    "    while identities:\n",
    "        print('Identities left to process: ', end='')\n",
    "        print(', '.join(f'{identity[0]} = {identity[1]}'\n",
    "                            for identity in identities\n",
    "                       )\n",
    "             )\n",
    "        expression_1, expression_2 = identities.pop()\n",
    "        print('    Dealing with:', expression_1, '=', expression_2)\n",
    "        if expression_1.root == expression_2.root:\n",
    "            for (expr_1, expr_2) in zip(expression_1.children,\n",
    "                                        expression_2.children\n",
    "                                       ):\n",
    "                print('    Adding:', expr_1, '=', expr_2)\n",
    "                identities.append([expr_1, expr_2])\n",
    "        elif expression_1.is_variable():\n",
    "            # Occurrence check (omitted in most Prolog implementations)\n",
    "            if expression_1.root in expression_2.variables():\n",
    "                print(f'    {expression_1.root} occurs in {expression_2},',\n",
    "                      'giving up!'\n",
    "                     )\n",
    "                return\n",
    "            print('    Extending mgu with:', expression_1.root, '->',\n",
    "                  expression_2\n",
    "                 )\n",
    "            mgu[expression_1.root] = expression_2\n",
    "            for identity in identities:\n",
    "                print('    Changing (or not)', identity[0], '=', identity[1],\n",
    "                      'to ', end=''\n",
    "                     )\n",
    "                for i in range(2):\n",
    "                    identity[i] = identity[i].substitute_in_copy(\n",
    "                                            {expression_1.root: expression_2}\n",
    "                                                                )\n",
    "                print(identity[0], '=', identity[1])\n",
    "        elif expression_2.is_variable():\n",
    "            print('    Adding', expression_2, '=', expression_1)\n",
    "            identities.append([expression_2, expression_1])\n",
    "        else:\n",
    "            print('    Equality cannot be satisfied, giving up!')\n",
    "            return\n",
    "        print()\n",
    "    print('Mgu:')\n",
    "    print('\\n'.join(f'    {var} -> {mgu[var]}' for var in mgu))\n",
    "    return mgu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_1 = Term.parse_term('X')\n",
    "term_2 = Term.parse_term('X')\n",
    "mgu = trace_unify_identities([(term_1, term_2)])\n",
    "\n",
    "if mgu is not None:\n",
    "    print('\\nExpressions after application of mgu:')\n",
    "    print('   ', term_1.substitute(mgu))\n",
    "    print('   ', term_2.substitute(mgu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_1 = Term.parse_term('X')\n",
    "term_2 = Term.parse_term('a')\n",
    "mgu = trace_unify_identities([(term_1, term_2)])\n",
    "\n",
    "if mgu is not None:\n",
    "    print('\\nExpressions after application of mgu:')\n",
    "    print('   ', term_1.substitute(mgu))\n",
    "    print('   ', term_2.substitute(mgu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_1 = Term.parse_term('X')\n",
    "term_2 = Term.parse_term('Y')\n",
    "mgu = trace_unify_identities([(term_1, term_2)])\n",
    "\n",
    "if mgu is not None:\n",
    "    print('\\nExpressions after application of mgu:')\n",
    "    print('   ', term_1.substitute(mgu))\n",
    "    print('   ', term_2.substitute(mgu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_1 = Term.parse_term('f(X, Y)')\n",
    "term_2 = Term.parse_term('f(Y, X)')\n",
    "mgu = trace_unify_identities([(term_1, term_2)])\n",
    "\n",
    "if mgu is not None:\n",
    "    print('\\nExpressions after application of mgu:')\n",
    "    print('   ', term_1.substitute(mgu))\n",
    "    print('   ', term_2.substitute(mgu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_1 = Term.parse_term('f(X1, h(X1), X2)')\n",
    "term_2 = Term.parse_term('f(g(X3), X4, X3)')\n",
    "mgu = trace_unify_identities([(term_1, term_2)])\n",
    "\n",
    "if mgu is not None:\n",
    "    print('\\nExpressions after application of mgu:')\n",
    "    print('   ', term_1.substitute(mgu))\n",
    "    print('   ', term_2.substitute(mgu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_1 = Term.parse_term('f(X1 ,g(X2, X3), X2, b)')\n",
    "term_2 = Term.parse_term('f(g(h(a, X5), X2), X1, h(a, X4), X4)')\n",
    "mgu = trace_unify_identities([(term_1, term_2)])\n",
    "\n",
    "if mgu is not None:\n",
    "    print('\\nExpressions after application of mgu:')\n",
    "    print('   ', term_1.substitute(mgu))\n",
    "    print('   ', term_2.substitute(mgu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_1 = Term.parse_term('f(X, a)')\n",
    "term_2 = Term.parse_term('f(b, X)')\n",
    "mgu = trace_unify_identities([(term_1, term_2)])\n",
    "\n",
    "if mgu is not None:\n",
    "    print('\\nExpressions after application of mgu:')\n",
    "    print('   ', term_1.substitute(mgu))\n",
    "    print('   ', term_2.substitute(mgu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_1 = Term.parse_term('f(X, Y, U)')\n",
    "term_2 = Term.parse_term('f(Y, U, g(X))')\n",
    "mgu = trace_unify_identities([(term_1, term_2)])\n",
    "\n",
    "if mgu is not None:\n",
    "    print('\\nExpressions after application of mgu:')\n",
    "    print('   ', term_1.substitute(mgu))\n",
    "    print('   ', term_2.substitute(mgu))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implementation of the unification algorithm in `Expression` should now be clear:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Expression(Expression):\n",
    "    def unify_identities(identities):\n",
    "        mgu = {}\n",
    "        while identities:\n",
    "            expression_1, expression_2 = identities.pop()\n",
    "            if expression_1.root == expression_2.root:\n",
    "                identities.extend([expr_1, expr_2] for (expr_1, expr_2) in\n",
    "                                                     zip(expression_1.children,\n",
    "                                                         expression_2.children\n",
    "                                                        )\n",
    "                                 )\n",
    "            elif expression_1.is_variable():\n",
    "                # Occurrence check (omitted in most Prolog\n",
    "                # implementations)\n",
    "                if expression_1.root in expression_2.variables():\n",
    "                    return\n",
    "                mgu[expression_1.root] = expression_2\n",
    "                for identity in identities:\n",
    "                    for i in range(2):\n",
    "                        identity[i] = identity[i].substitute_in_copy(\n",
    "                                              {expression_1.root: expression_2}\n",
    "                                                                    )\n",
    "            elif expression_2.is_variable():\n",
    "                identities.append([expression_2, expression_1])\n",
    "            else:\n",
    "                return\n",
    "        return mgu\n",
    "            \n",
    "    def unify(self, expression):\n",
    "        return Expression.unify_identities([[self, expression]])\n",
    "\n",
    "\n",
    "class Term(Term, Expression):\n",
    "    pass\n",
    "\n",
    "\n",
    "class Atom(Atom, Expression):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term = Term.parse_term('X')\n",
    "mgu = term.unify(Term.parse_term('X'))\n",
    "{var: str(mgu[var]) for var in mgu}\n",
    "\n",
    "term = Term.parse_term('X')\n",
    "mgu = term.unify(Term.parse_term('a'))\n",
    "{var: str(mgu[var]) for var in mgu}\n",
    "\n",
    "term = Term.parse_term('X')\n",
    "mgu = term.unify(Term.parse_term('Y'))\n",
    "{var: str(mgu[var]) for var in mgu}\n",
    "\n",
    "term = Term.parse_term('f(X, Y)')\n",
    "mgu = term.unify(Term.parse_term('f(Y, X)'))\n",
    "{var: str(mgu[var]) for var in mgu}\n",
    "\n",
    "term = Term.parse_term('f(X1, h(X1), X2)')\n",
    "mgu = term.unify(Term.parse_term('f(g(X3), X4, X3)'))\n",
    "{var: str(mgu[var]) for var in mgu}\n",
    "\n",
    "term = Term.parse_term('f(X1 ,g(X2, X3), X2, b)')\n",
    "mgu = term.unify(Term.parse_term('f(g(h(a, X5), X2), X1, h(a, X4), X4)'))\n",
    "{var: str(mgu[var]) for var in mgu}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have all functions and methods we need to build and operate on atoms and terms. We can now further compose:\n",
    "\n",
    "* conjunctions of atoms as rule bodies;\n",
    "* rule heads (atoms) and rule bodies as rules;\n",
    "* sequences of rules as logic programs.\n",
    "\n",
    "We define a new class for each: `Conjunction`, `Rule`, and `LogicProgram`.\n",
    "\n",
    "A `Conjunction` object is intended to be a list of `Atom` objects; this is achieved by letting `Conjunction` inherit from `list`. We can then define in `Conjunction` a method, `predicate_and_function_symbols()`, meant to collect all predicate and function symbols that occur in the atoms that make up a conjunction, while checking that they are all used in a consistent manner:\n",
    "\n",
    "* no predicate symbol and no function symbol should be used with different arities in different atoms;\n",
    "* no symbol should be used as a predicate symbol in an atom and as a function symbol in another atom.\n",
    "\n",
    "The implementation of `predicate_and_function_symbols()` in `Conjunction` is similar to the implementation of `collected_symbols()` in `Expression`, with at their hearts uses of `consistently_add_to()` and `consistently_merge_to()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conjunction(list):\n",
    "    class ConjunctionError(Exception):\n",
    "        pass\n",
    "\n",
    "    def __init__(self, conjuncts):\n",
    "        super().__init__(conjuncts)\n",
    "\n",
    "    def predicate_and_function_symbols(self):\n",
    "        predicate_symbols = {}\n",
    "        function_symbols = {}\n",
    "        for atom in self:\n",
    "            atom_predicate_symbol, atom_function_symbols =\\\n",
    "                    atom.predicate_and_function_symbols()\n",
    "            if not consistently_add_to(atom_predicate_symbol, predicate_symbols\n",
    "                                      ):\n",
    "                raise Conjunction.ConjunctionError(f'{self}: predicate symbol '\n",
    "                                                   'used with many arities'\n",
    "                                                  )\n",
    "            if not consistently_merge_to(atom_function_symbols,\n",
    "                                         function_symbols\n",
    "                                        ):\n",
    "                raise Conjunction.ConjunctionError(f'{self}: function symbol '\n",
    "                                                   'used with many arities'\n",
    "                                                  )\n",
    "        if predicate_symbols.keys() & function_symbols.keys():\n",
    "            raise Conjunction.ConjunctionError(f'{self}: symbol used as '\n",
    "                                               'predicate and function symbol'\n",
    "                                              )\n",
    "        return predicate_symbols, function_symbols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we implement the `Rule` class, with three attributes: `head`, meant to denote an `Atom` object, `body`, meant to denote a `Conjunction` object, and `variables`, meant to denote the set of variables occurring in the atoms that make up a rule. `Conjunction` has a `predicate_and_function_symbols()` method; `Rule` also has a method with that name, similarly implemented, meant to collect all predicate and function symbols that occur in a rule, while checking that:\n",
    "\n",
    "* neither the predicate symbol nor any function symbol in the rule's head is used in the rule's body with a different arity;\n",
    "* no symbol is used as a predicate symbol in the rule's head and as a function symbol in the rule's body, and the other way around.\n",
    "\n",
    "We implement in `Rule` the `__str__()` method to display a rule, properly formatted. Finally, we implement a function, `parse_rule()`, similar to the `parse_term()` and `parse_atom()` functions of the `Term` and `Atom` classes, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rule:\n",
    "    class RuleError(Exception):\n",
    "        pass\n",
    "\n",
    "    def __init__(self, head, body=[]):\n",
    "        self.head = head\n",
    "        self.body = body\n",
    "        self.variables =  self.head.variables()\n",
    "        for atom in self.body:\n",
    "            self.variables |= atom.variables() \n",
    "\n",
    "    def __str__(self):\n",
    "        if not self.body:\n",
    "            return self.head.__str__() + '.'\n",
    "        return ' :- '.join((self.head.__str__(),\n",
    "                            ', '.join(atom.__str__() for atom in self.body)\n",
    "                           )\n",
    "                          ) + '.'\n",
    "\n",
    "    def predicate_and_function_symbols(self):\n",
    "        head_predicate_symbol, head_function_symbols =\\\n",
    "                self.head.predicate_and_function_symbols()\n",
    "        if not self.body:\n",
    "            return {head_predicate_symbol[0]: head_predicate_symbol[1]},\\\n",
    "                   head_function_symbols\n",
    "        predicate_symbols, function_symbols =\\\n",
    "                self.body.predicate_and_function_symbols()        \n",
    "        if not consistently_add_to(head_predicate_symbol, predicate_symbols):\n",
    "            raise Rule.RuleError(f\"{self}: head's predicate symbol used with \"\n",
    "                                 'different arity in body'\n",
    "                                )\n",
    "        if not consistently_merge_to(head_function_symbols, function_symbols):\n",
    "            raise Rule.RuleError(f'{self}: function symbol used with '\n",
    "                                 'different arities in head and body'\n",
    "                                )\n",
    "        if predicate_symbols.keys() & function_symbols.keys():\n",
    "            raise Rule.RuleError(f'{self}: symbol used as predicate and '\n",
    "                                 'function symbols, one in head, the other in '\n",
    "                                 'body'\n",
    "                                )\n",
    "        return predicate_symbols, function_symbols\n",
    "\n",
    "    def parse_rule(expression):\n",
    "        if expression[-1] != '.':\n",
    "            raise Rule.RuleError(f'{expression}: does not end in a full stop')\n",
    "        rule = expression[: -1].split(':-')\n",
    "        if not (1 <= len(rule) <= 2):\n",
    "            raise Rule.RuleError(f'{expression}: syntactically invalid')\n",
    "        head = Expression.parse_item(rule[0])\n",
    "        if not head:\n",
    "            raise Rule.RuleError(f'{expression}: syntactically incorrect head')\n",
    "        if len(rule) == 1:\n",
    "            rule = Rule(head)\n",
    "        else:\n",
    "            body = Expression.parse_item(rule[1], parse_single_subitem=False)\n",
    "            if not body:\n",
    "                raise Rule.RuleError(f'{expression}: syntactically incorrect '\n",
    "                                     'body'\n",
    "                                    )\n",
    "            rule = Rule(head, Conjunction(body))\n",
    "        rule.predicate_and_function_symbols()\n",
    "        return rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Rule.parse_rule('female(alice).'))\n",
    "\n",
    "print(Rule.parse_rule('sisterof(X, Y) :- parents(X, M, F), female(X),'\n",
    "                      ' parents(Y, M, F).'\n",
    "                     )\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we implement that part of the `LogicProgram` class that creates a logic program object from the contents of a file, with three attributes: `program`, meant to denote a list of `Rule` objects, and `predicate_symbols` and `function_symbols`, meant to denote the sets of predicate and function symbols, respectively, occurring in the rules that make up the logic program. The `__init__()` method of `LogicProgram` computes `predicate_symbols` and `function_symbols` in a way that is similar to the implementation of the `predicate_and_function_symbols()` method of the `Conjunction` and `Rule` classes, while checking that:\n",
    "\n",
    "* no predicate symbol or function symbol is used with different arities in different rules;\n",
    "* no symbol is used both as a predicate symbol and as a function symbol in different rules.\n",
    "\n",
    "Prolog comments start with `%`; lines in the file whose first nonspace symbol is `%`, as well as blank lines, are ignored.\n",
    "\n",
    "The `individualise_underscores()` method of the `Expression` class is used to, for each rule, replace every occurrence of `_` in the rule as a new, unique variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogicProgram:\n",
    "    class LogicProgramError(Exception): \n",
    "        pass\n",
    "\n",
    "    def __init__(self, filename):\n",
    "        self.program = []\n",
    "        self.predicate_symbols = {}\n",
    "        self.function_symbols = {}\n",
    "        rule_nb = 0\n",
    "        with open(filename) as file:\n",
    "            for rule in file:\n",
    "                rule = rule.strip()\n",
    "                if not rule or rule.startswith('%'):\n",
    "                    continue\n",
    "                rule = Rule.parse_rule(rule)\n",
    "                rule_nb += 1\n",
    "                rule_predicate_symbols, rule_function_symbols =\\\n",
    "                        rule.predicate_and_function_symbols()\n",
    "                if not consistently_merge_to(rule_predicate_symbols,\n",
    "                                             self.predicate_symbols\n",
    "                                            )\\\n",
    "                   or not consistently_merge_to(rule_function_symbols,\n",
    "                                                self.function_symbols\n",
    "                                               ):\n",
    "                    raise LogicProgram.LogicProgramError(\n",
    "                             f'Symbol arity in rule nb {rule_nb} '\n",
    "                            'inconsistent with arities in previous rules'\n",
    "                                                        )\n",
    "                rule_variables = rule.head.variables()\n",
    "                for atom in rule.body:\n",
    "                    rule_variables |= atom.variables()\n",
    "                underscore_index =\\\n",
    "                      rule.head.individualise_underscores(rule_variables)\n",
    "                for atom in rule.body:\n",
    "                    underscore_index =\\\n",
    "                            atom.individualise_underscores(rule_variables,\n",
    "                                                           underscore_index\n",
    "                                                          )\n",
    "                self.program.append(rule)\n",
    "        if self.predicate_symbols.keys() & self.function_symbols.keys():\n",
    "            raise LogicProgram.LogicProgramError('Symbol used as predicate '\n",
    "                                                 'and function symbols'\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything is now in place for the Prolog interpreter to solve queries, with an exploration of the search tree that defaults to depth-first, but that can be changed to breadth-first. The following tracing function, `trace_solve()`, is meant to explain and illustrate the workings of the interpreter. The second argument, `query`, is meant to be a string that represents a single atom or a sequence of atoms separated by spaces, to be interpreted as a goal or an implicitly conjuncted sequence of goals; the `parse_item()` function from the `Expression` class, with the argument `parse_single_subitem` set to `False`, is all we need to convert `query` to a list of `Atom` elements, one for each goal. The only place where depth-first and breadth-first explorations differ is at the very end of the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trace_solve(logic_program, query, depth_first=True):\n",
    "    query = Conjunction(Expression.parse_item(query,\n",
    "                                              parse_single_subitem=False\n",
    "                                             )\n",
    "                       )\n",
    "    query_variables = {var for atom in query for var in atom.variables()}\n",
    "    # A list of pairs consisting of:\n",
    "    # - a list of goals to be solved, and\n",
    "    # - the substitution to apply to the variables that occur in the\n",
    "    #   query as determined by the unifications computed so far.\n",
    "    goals_solution_pairs =\\\n",
    "            deque([(deque(query), {var: Term(var) for var in query_variables})\n",
    "                  ]\n",
    "                 )\n",
    "    while goals_solution_pairs:\n",
    "        goals, solution = goals_solution_pairs.popleft()\n",
    "        print('\\nDealing with following goals and partial solution:')\n",
    "        print('   ', ', '.join(goal.__str__() for goal in goals), end='')\n",
    "        print('   ', ', '.join(f'{var} -> {solution[var]}'\n",
    "                                    for var in solution)\n",
    "             )\n",
    "        if not goals:\n",
    "            print('    No goal left, solution is complete')\n",
    "            yield {var: solution[var].__str__() for var in solution}\n",
    "            continue\n",
    "        reserved_variables =\\\n",
    "           query_variables |{var for atom in goals for var in atom.variables()}\n",
    "        goal = goals.popleft()\n",
    "        next_goals_solution_pairs = deque()\n",
    "        for rule in logic_program.program:\n",
    "            variable_renaming = Expression.fresh_variables(rule.variables,\n",
    "                                                           reserved_variables\n",
    "                                                          )\n",
    "            head = rule.head.rename_variables(variable_renaming)\n",
    "            mgu = goal.unify(head)\n",
    "            if mgu is not None:\n",
    "                print('    First goal unified with head of rule:', rule)\n",
    "                print('      Renaming variables, head becomes:', head)\n",
    "                print('      Mgu:',\n",
    "                      ', '.join(f'{var} -> {mgu[var]}' for var in mgu)\n",
    "                     )\n",
    "                new_goals =\\\n",
    "                    deque(atom.rename_variables(variable_renaming)\\\n",
    "                              .substitute(mgu) for atom in rule.body\n",
    "                         )\n",
    "                new_goals.extend(goal.substitute_in_copy(mgu) for goal in goals\n",
    "                                )\n",
    "                if new_goals:\n",
    "                    print('      New goals to solve: ', end='')\n",
    "                    print(', '.join(goal.__str__() for goal in new_goals))\n",
    "                    print('      New partial solution:',\n",
    "                          ', '.join(f'{var} -> '\n",
    "                                    f'{solution[var].substitute_in_copy(mgu)}'\n",
    "                                            for var in solution\n",
    "                                   )\n",
    "                         )\n",
    "\n",
    "                next_goals_solution_pairs.append(\n",
    "                                   (new_goals,\n",
    "                                    {var: solution[var].substitute_in_copy(mgu)\n",
    "                                         for var in solution\n",
    "                                    }\n",
    "                                   )\n",
    "                                                )\n",
    "        if depth_first:\n",
    "            goals_solution_pairs.extendleft(reversed(next_goals_solution_pairs)\n",
    "                                           )\n",
    "        else:\n",
    "            goals_solution_pairs.extend(next_goals_solution_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat prolog_ex_2.pl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tracing execution for a closed goal, so checking that the goal is a logical consequence of the logic program:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logic_program = LogicProgram('prolog_ex_2.pl')\n",
    "for _ in trace_solve(logic_program, 'grandparent(john, jack)'):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tracing the depth-first search exploration for solutions to the goal `grandparent(john, X)` as illustrated with the first tree above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logic_program = LogicProgram('prolog_ex_2.pl')\n",
    "for _ in trace_solve(logic_program, 'grandparent(john, X)'):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the same but changing exploration to breadth-first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logic_program = LogicProgram('prolog_ex_2.pl')\n",
    "for _ in trace_solve(logic_program, 'grandparent(john, X)', False):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat prolog_ex_5.pl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tracing the depth-first search exploration for solutions to the goal `join(X, X, Y)` as illustrated with the third tree above yields the following. As there are infinitely many solutions, we use the `islice` class from the `itertools` module to trace the search for the first 3 solutions only: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logic_program = LogicProgram('prolog_ex_5.pl')\n",
    "for _ in islice(trace_solve(logic_program, 'join(X, X, Y)'), 3):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implementation of the Prolog interpreter in `LogicProgram` should now be clear. The only addition to the code of the tracing function is that we check that the goals to solve are built from predicate and function symbols that all occur in the logic program:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogicProgram(LogicProgram):\n",
    "    def solve(self, query, depth_first=True):\n",
    "        query = Conjunction(Expression.parse_item(query,\n",
    "                                                  parse_single_subitem=False\n",
    "                                                 )\n",
    "                           )\n",
    "        query_predicate_symbols, query_function_symbols =\\\n",
    "                query.predicate_and_function_symbols()\n",
    "        if any(predicate_symbol not in self.predicate_symbols\n",
    "               or query_predicate_symbols[predicate_symbol]\n",
    "               != self.predicate_symbols[predicate_symbol]\n",
    "                    for predicate_symbol in query_predicate_symbols\n",
    "              ):\n",
    "            raise LogicProgram.QuerryError(f'{query}: predicate symbol '\n",
    "                                           'in query not in program'\n",
    "                                          )\n",
    "        if any(function_symbol not in self.function_symbols\n",
    "               or query_function_symbols[function_symbol]\n",
    "               != self.function_symbols[function_symbol]\n",
    "                    for function_symbol in query_function_symbols\n",
    "              ):\n",
    "            raise LogicProgram.QuerryError(f'{query}: function symbol '\n",
    "                                           'in query not in program'\n",
    "                                          )               \n",
    "        query_variables = {var for atom in query for var in atom.variables()}\n",
    "        # A list of pairs consisting of:\n",
    "        # - a list of goals to be solved, and\n",
    "        # - the substitution to apply to the variables that occur in the\n",
    "        #   query as determined by the unifications computed so far.\n",
    "        goals_solution_pairs = deque([(deque(query),\n",
    "                                       {var: Term(var)\n",
    "                                            for var in query_variables\n",
    "                                       }\n",
    "                                      )\n",
    "                                     ]\n",
    "                                    )\n",
    "        while goals_solution_pairs:\n",
    "            goals, solution = goals_solution_pairs.popleft()\n",
    "            if not goals:\n",
    "                yield {var: solution[var].__str__() for var in solution}\n",
    "                continue\n",
    "            reserved_variables = query_variables\\\n",
    "                                 | {var for atom in goals\n",
    "                                           for var in atom.variables()\n",
    "                                   }\n",
    "            goal = goals.popleft()\n",
    "            next_goals_solution_pairs = deque()\n",
    "            for rule in self.program:\n",
    "                variable_renaming =\\\n",
    "                        Expression.fresh_variables(rule.variables,\n",
    "                                                   reserved_variables\n",
    "                                                  )\n",
    "                head = rule.head.rename_variables(variable_renaming)\n",
    "                mgu = goal.unify(head)\n",
    "                if mgu is not None:\n",
    "                    new_goals = deque(atom.rename_variables(variable_renaming)\n",
    "                                         .substitute(mgu) for atom in rule.body\n",
    "                                     )\n",
    "                    new_goals.extend(goal.substitute_in_copy(mgu)\n",
    "                                          for goal in goals\n",
    "                                    )\n",
    "                    next_goals_solution_pairs.append(\n",
    "                                   (new_goals,\n",
    "                                    {var: solution[var].substitute_in_copy(mgu)\n",
    "                                         for var in solution\n",
    "                                    }\n",
    "                                   )\n",
    "                                                    )\n",
    "            if depth_first:\n",
    "                goals_solution_pairs.extendleft(\n",
    "                                        reversed(next_goals_solution_pairs)\n",
    "                                               )\n",
    "            else:\n",
    "                goals_solution_pairs.extend(next_goals_solution_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LP = LogicProgram('prolog_ex_2.pl')\n",
    "for solution in LP.solve('grandparent(john, X)'):\n",
    "        print('   ', solution)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
